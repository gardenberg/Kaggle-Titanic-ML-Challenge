{
    "collab_server" : "",
    "contents" : "#Rscripts from datacamp tutorial - https://campus.datacamp.com/courses/kaggle-r-tutorial-on-machine-learning/\n#also includes feature engineering from Megan Risdal - https://www.kaggle.com/mrisdal/titanic/exploring-survival-on-the-titanic\n\n# Assign the training set\ntrain <- read.csv(url(\"http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/train.csv\"))\n\n# Assign the testing set\ntest <- read.csv(url(\"http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/test.csv\"))\n\n# Create the column child, and indicate whether child or no child\ntrain$Child <- NA\ntrain$Child[train$Age<18] <- 1\ntrain$Child[train$Age>=18] <- 0\n\n# Two-way comparison\ntable(train$Child, train$Survived)\nprop.table(table(train$Child, train$Survived),1)\n\n#baseline model: all women survives\n\n# Decision Trees\nlibrary(rpart)\n\n# Build the decision tree\nmy_tree_two <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, data=train, method=\"class\")\n\n# Visualize the decision tree using plot() and text()\nplot(my_tree_two)\ntext(my_tree_two)\n\n# Load in the packages to create a fancified version of your tree\n\nlibrary(rattle)\nlibrary(rpart.plot)\nlibrary(RColorBrewer)\n\n# Time to plot your fancified tree\nfancyRpartPlot(my_tree_two)\nmy_prediction <- predict(my_tree_two, test, type=\"class\") \n\n# Create a data frame with two columns: PassengerId & Survived. Survived contains your predictions\nmy_solution <- data.frame(PassengerId = test$PassengerId, Survived = my_prediction)\n\n# Check that your data frame has 418 entries\nnrow(my_solution)\n\n# Write your solution to a csv file \nwrite.csv(my_solution, file=\"my_datacamp2_solution.csv\", row.names=F)\n\n\n# Create a new decision tree my_tree_three\nmy_tree_three <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, data=train, method=\"class\", control=rpart.control(minsplit=50, cp=0) )\n\n#overfitting\n#minsplit = 2 er minste leafnode mulig, cp betyr ingen splitstop. dette bør gi en overfitta modell\nmy_tree_three <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, \n                       data = train, method = \"class\", control = rpart.control(minsplit = 2, cp = 0))\n\n# Visualize my_tree_three\nfancyRpartPlot(my_tree_three)\n\n#minsplit = 2 er minste leafnode mulig, cp betyr ingen splitstop. dette bør gi en overfitta modell\nmy_tree_three <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, \n                       data = train, method = \"class\", control = rpart.control(minsplit = 50, cp = 0))\n\n# Visualize my_tree_three\nfancyRpartPlot(my_tree_three)\n\n#feature engineering - legger inn en variabel for familiestørrelse\ntrain_two <- train\n\ntrain_two$family_size <- train_two$SibSp+train_two$Parch+1\n\n# Create a new decision tree \nmy_tree_four <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + family_size, data=train_two, method=\"class\")\n\n#Add title-variable\ntrain_new=train\ntest_new=test\ntrain_new$Title <- gsub('(.*, )|(\\\\..*)', '', train_new$Name)\ntest_new$Title <- gsub('(.*, )|(\\\\..*)', '', test_new$Name)\n\n# Titles with very low cell counts to be combined to \"rare\" level\nrare_title <- c('Dona', 'Lady', 'the Countess','Capt', 'Col', 'Don', \n                'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer')\n\n# Also reassign mlle, ms, and mme accordingly\ntrain_new$Title[train_new$Title == 'Mlle']        <- 'Miss' \ntrain_new$Title[train_new$Title == 'Ms']          <- 'Miss'\ntrain_new$Title[train_new$Title == 'Mme']         <- 'Mrs' \ntrain_new$Title[train_new$Title %in% rare_title]  <- 'Rare Title'\ntest_new$Title[test_new$Title == 'Mlle']        <- 'Miss' \ntest_new$Title[test_new$Title == 'Ms']          <- 'Miss'\ntest_new$Title[test_new$Title == 'Mme']         <- 'Mrs' \ntest_new$Title[test_new$Title %in% rare_title]  <- 'Rare Title'\n\n# Show title counts by sex again\ntable(train_new$Sex, train_new$Title)\n\nstr(train_new)\nstr(test_new$Title)\n\n# Create a new decision tree\nmy_tree_five <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + Title, data=train_new, method=\"class\")\n\n# Make your prediction using `my_tree_five` and `test_new`\nmy_prediction <- predict(my_tree_five, test_new, type=\"class\")\n\n# Create a data frame with two columns: PassengerId & Survived. Survived contains your predictions\nmy_solution <- data.frame(PassengerId = test_new$PassengerId, Survived = my_prediction)\n\n# Check that your data frame has 418 entries\nnrow(my_solution)\n\n# Write your solution to a csv file with the name my_solution.csv\nwrite.csv(my_solution, file=\"my_datacamp3_solution.csv\", row.names=F)\n\n\n# Random Forest\n#It grows multiple (very deep) classification trees using the training set. \n#At the time of prediction, each tree is used to come up with a prediction and every outcome is counted as a vote. \n#For example, if you have trained 3 trees with 2 saying a passenger in the test set will survive and 1 says he will not, the passenger will be classified as a survivor. \n#This approach of overtraining trees, but having the majority's vote count as the actual classification decision, avoids overfitting.\n#requires no missing data!\n\n# cleaning up\n# All data, both training and test set\nall_data = rbind(train_new,test_new)\n\n# Passenger on row 62 and 830 do not have a value for embarkment. \n# Since many passengers embarked at Southampton, we give them the value S.\n# We code all embarkment codes as factors.\nall_data$Embarked[c(62,830)] = \"S\"\nall_data$Embarked <- factor(combi$Embarked)\n\n# Passenger on row 1044 has an NA Fare value. Let's replace it with the median fare value.\nall_data$Fare[1044] <- median(combi$Fare, na.rm=TRUE)\n\n# How to fill in missing Age values?\n# We make a prediction of a passengers Age using the other variables and a decision tree model. \n# This time you give method=\"anova\" since you are predicting a continuous variable.\npredicted_age <- rpart(Age ~ Pclass + Sex + SibSp + Parch + Fare + Embarked + Title + family_size,\n                       data=all_data[!is.na(all_data$Age),], method=\"anova\")\nall_data$Age[is.na(all_data$Age)] <- predict(predicted_age, all_data[is.na(all_data$Age),])\n\n# Split the data back into a train set and a test set\ntrain <- all_data[1:891,]\ntest <- all_data[892:1309,]\n\n#One more important element in Random Forest is randomization to avoid the creation of the same tree from the training set. You randomize in two ways: by taking a randomized sample of the rows in your training set and by only working with a limited and changing number of the available variables for every node of the tree.\n\n# Load in the package\nlibrary(randomForest)\n\n# Train set and test set\nstr(train)\nstr(test)\n\n# Set seed for reproducibility\nset.seed(111)\n\n# Apply the Random Forest Algorithm\nmy_forest <- randomForest(as.factor(Survived) ~ Pclass + Sex + Age +  SibSp + Parch + Fare + Embarked + Title, data=train, importance=T, ntree=1000)\n\n# Make your prediction using the test set\nmy_prediction <- predict(my_forest, test)\n\n# Create a data frame with two columns: PassengerId & Survived. Survived contains your predictions\nmy_solution <- data.frame(PassengerId = test$PassengerId, Survived = my_prediction)\n\nwrite.csv(my_solution, file=\"my_datacamp4_solution.csv\", row.names=F)\n\n#not better than datacamp3, actually...\n\nvarImpPlot(my_forest)\n",
    "created" : 1473967159347.000,
    "dirty" : true,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3653129610",
    "id" : "FB880C32",
    "lastKnownWriteTime" : 1473970429,
    "last_content_update" : -2147483648,
    "path" : "D:/R/kaggle_titanic/datacamp_tutorial.R",
    "project_path" : "datacamp_tutorial.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}